# 听课记录

## 04 数据操作 + 数据预处理

N位数组样例

标量

向量

矩阵

3d 一张RGB的图片 （宽度、高度、通道）

4d 一个RGB图片的批量（批量、宽度、高度、通道）

5d 一个视频批量（时间、批量、宽度、高度、通道）

创建数组

访问元素

数据操作

导入 torch

reshape修改张量的形状

zeros ones 

创建数组后可以使用标准的算数运算

也可以把多个张量连接在一起

逻辑运算符 构建二元张量

广播机制（最容易出错的地方）

id 内存 指针

数据预处理实现

如果有一个原始数据，怎么读取用于之后进行处理

创建一个人工数据集，存储在csv（逗号分隔值）文件中

从创建的csv文件中加载原始数据集 使用pandas

read_csv

为了处理缺失的数据：删除、插值

将NaN视为一个类别

## 05 线性代数

 标量由只有一个元素的张量表示

将向量视为标量值组成的列表

通过张量的索引来访问任意元素

访问张量的长度

只有一个轴的张量，形状只有一个元素

通过指定两个分量m和n来小黄见一个形状位mxn的矩阵

矩阵的转置

对称矩阵

转置等于自己

就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建由更多轴的数据结构

两个矩阵的按元素乘法称为 哈达玛积

计算其元素的和

表示任意形状的张量的元素和

指定求和汇总张量的值

一个与求和相关的量是 平均值

计算总和

点积是相同位置的暗元素成绩的和

我i们可以他用过执行暗元素乘法

矩阵向量机是一个长度位m的裂像力啊ing，

我们可以将矩阵矩阵乘法看作是简单的执行m次矩阵向量机，并将

L2范数 是向量元素平法和的平方根

norm

L1范数 表示位向量元素的

矩阵的 F范数 矩阵元素的平方和的平方根

按特定轴求和

假设有个矩阵 5行4列

shape: [5,4]

axis: 0, 1

## 矩阵计算

标量导数

导数是切线的斜率

将导数扩展到不可微的函数

亚导数

梯度

将导数拓展到向量

注意形状

分子布局符号（一般使用）

分母布局符号

## 自动求导

向量链式法则

标量链式法则

拓展到向量

将代码分解成操作子

将在计算表示成一个无环图

显示构造

隐式构造

自动求导的两种模式

链式法则

正向累计

反向了累计、又称反向传递

求梯度的结果需要一直存储下来，导致对GPU的内存要求大

arange

需要一个地方存储梯度

requires_grad_(True)

dot 内积

通过调用反向传播函数来计算y关于x每个分量的梯度

计算x的另一个函数

默认情况下，Pytorch会累计梯度，我们需要清楚之前的值

在深度学习中，我们的目的不是计算微分矩阵，而是批量中每个样本单独计算的偏导数之和

对标量求导。一般的

将某些计算移动到记录的计算图以外

隐式构造 和 显示构造

需要正向和反向都算一遍

为什么Pytorch会默认累计梯度？是因为Pytorch对内存的管理不是那么好

为什么深度学习都是对标量求导？而不是矩阵或者向量？因为loss通常是标量

求导过程 图结构

 ## 线性回归 + 基础优化算法

在美国买房

衡量预估质量

真实值减去估计值的平方 平方误差

训练数据

权重和偏差

采集数据

训练数据

越多越好

假设我们有n个样本

训练损失

最小化损失来学习参数

线性回归是对n维输入的加权，外加偏差

使用平方损失来衡量预测值和真实值额

线性回归有显示解

线性回归可以看作是单层的神经网络

基础优化方法

梯度下降

挑选一个初始值（随机）

重复迭代参数

选择学习率

不能太小也不能太大

计算梯度很贵（几乎是最贵的）

选择合适的学习率

在整个训练集上算梯度太贵

对整个损失函数求导

求一次梯度需要对整个样本进行

计算代价很大

近似的办法是所有的样本的损失的平均

随机采样b个样本来近似损失

b是批量大小

太小的话，不适合并行来最大利用计算资源

太大内存消耗增加，浪费计算，例如如果所有的样本都是相同的

梯度下降 通过不断验证发梯度方向更新参数求解

小批量随机梯度下降是深度学习默认的求解方法（最稳定的方法）

两个重要的超参数是批量大小和学习率

线性回归的从零开始实现

首先，我们将从零开始实现整个方法

包括数据流水线，模型、损失函数和小批量随机梯度下降优化器

```python
import random 
import torch
from d2l import torch as d2l
```

根据带有噪声的线性模型构造一个人造数据集

我们使用线性模型参数和早射二哥生成数据集机器标签

定义初始化模型参数

定义模型

定义损失函数：均方误差

定义优化算法：小批量随机梯度下降

训练过程

比较真丝参数和通过训练学到的参数来评估训练的成功度

超参数的选择

线性回归的简介实现

通过使用深度学习框架来简介的实现 线性回归模型 生成数据集

```python
import numpy
```

调用框架中现有等的API来读取数据据

构造一个PyTorch数据迭代器

使用框架定义好的层

初始化模型参数

计算均方误差使用的MSELoss，也称平方L2范数

实例化SGD示例

SGD

训练过程的代码与我们从零开始实现是所作的非常相似

 Google colab

为什么使用平方损失而不是绝对差值？其实区别不大，但是绝对差值是一个不可导的，导数比较难求

损失为什么要求平均？本质上没有关系，数值上是一样的，但是

如何找到合适的学习率？

一定的噪音就像是对小孩不是一直夸夸教育

过拟合和欠拟合的情况下，学习率和批次该如何进行调整？有什么常见的策略？

针对批量大小的数据集进行网络线性

