# 听课记录

## 04 数据操作 + 数据预处理

N位数组样例

标量

向量

矩阵

3d 一张RGB的图片 （宽度、高度、通道）

4d 一个RGB图片的批量（批量、宽度、高度、通道）

5d 一个视频批量（时间、批量、宽度、高度、通道）

创建数组

访问元素

数据操作

导入 torch

reshape修改张量的形状

zeros ones 

创建数组后可以使用标准的算数运算

也可以把多个张量连接在一起

逻辑运算符 构建二元张量

广播机制（最容易出错的地方）

id 内存 指针

数据预处理实现

如果有一个原始数据，怎么读取用于之后进行处理

创建一个人工数据集，存储在csv（逗号分隔值）文件中

从创建的csv文件中加载原始数据集 使用pandas

read_csv

为了处理缺失的数据：删除、插值

将NaN视为一个类别

## 05 线性代数

 标量由只有一个元素的张量表示

将向量视为标量值组成的列表

通过张量的索引来访问任意元素

访问张量的长度

只有一个轴的张量，形状只有一个元素

通过指定两个分量m和n来小黄见一个形状位mxn的矩阵

矩阵的转置

对称矩阵

转置等于自己

就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建由更多轴的数据结构

两个矩阵的按元素乘法称为 哈达玛积

计算其元素的和

表示任意形状的张量的元素和

指定求和汇总张量的值

一个与求和相关的量是 平均值

计算总和

点积是相同位置的暗元素成绩的和

我i们可以他用过执行暗元素乘法

矩阵向量机是一个长度位m的裂像力啊ing，

我们可以将矩阵矩阵乘法看作是简单的执行m次矩阵向量机，并将

L2范数 是向量元素平法和的平方根

norm

L1范数 表示位向量元素的

矩阵的 F范数 矩阵元素的平方和的平方根

按特定轴求和

假设有个矩阵 5行4列

shape: [5,4]

axis: 0, 1

## 矩阵计算

标量导数

导数是切线的斜率

将导数扩展到不可微的函数

亚导数

梯度

将导数拓展到向量

注意形状

分子布局符号（一般使用）

分母布局符号

## 自动求导

向量链式法则

标量链式法则

拓展到向量

将代码分解成操作子

将在计算表示成一个无环图

显示构造

隐式构造

自动求导的两种模式

链式法则

正向累计

反向了累计、又称反向传递

求梯度的结果需要一直存储下来，导致对GPU的内存要求大

arange

需要一个地方存储梯度

requires_grad_(True)

dot 内积

通过调用反向传播函数来计算y关于x每个分量的梯度

计算x的另一个函数

默认情况下，Pytorch会累计梯度，我们需要清楚之前的值

在深度学习中，我们的目的不是计算微分矩阵，而是批量中每个样本单独计算的偏导数之和

对标量求导。一般的

将某些计算移动到记录的计算图以外

